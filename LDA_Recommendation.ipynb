{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA Recommendation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HongyuJiang/LDA-Recommendation/blob/master/LDA_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5jsER8oiVoXI",
        "colab_type": "code",
        "outputId": "cc4f0028-017e-4dba-9741-2a229e572098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t5OqLX_mXIJr",
        "colab_type": "code",
        "outputId": "4661aac7-4870-4510-985f-618ab44f9a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jzwmKbZWXLsG",
        "colab_type": "code",
        "outputId": "922ebc4b-e0f5-4793-b7b2-5c5f2834c55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "path_to_file = '/content/drive/My Drive/LDA Recommendation/access.txt'\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8').replace('\\ufeff','')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 189954288 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PqH0ZAdaXPJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = text.split('\\n')\n",
        "\n",
        "lines = lines[1:-1]\n",
        "lines_len = len(lines)\n",
        "courseDict = dict()\n",
        "\n",
        "userOperationDict = dict()\n",
        "\n",
        "for index in range(lines_len):\n",
        "  meta = lines[index].split(',')\n",
        "  user = meta[0]\n",
        "  courseDetail = meta[1].replace('http://www.xuetangx.com/courses/', '')\n",
        "  \n",
        "  char = '/'\n",
        "  \n",
        "  if courseDetail.find('%') != -1:\n",
        "    char = '%'\n",
        "  \n",
        "  linkSegment = courseDetail.split(char)\n",
        "  deep = len(linkSegment)\n",
        "  \n",
        "  courseName = ''\n",
        "  \n",
        "  if len(linkSegment[0].split(':')) > 1:\n",
        "    courseName = ('+'.join(linkSegment[0].split('+')[0:2])).split(':')[1]\n",
        "  else:\n",
        "    courseName = '+'.join(linkSegment[0].split('+')[0:2])\n",
        "      \n",
        "  courseDict[courseName] = 1\n",
        "  \n",
        "  courseChild = linkSegment[1]\n",
        "  \n",
        "  Optype = 'watch'\n",
        "  \n",
        "  if deep == 2 and courseChild == 'about':\n",
        "    Optype = 'intro'\n",
        "  \n",
        "  if user in userOperationDict:\n",
        "    if courseName in userOperationDict[user]:\n",
        "      if Optype in userOperationDict[user][courseName]:\n",
        "        userOperationDict[user][courseName][Optype] += 1\n",
        "      else:\n",
        "        userOperationDict[user][courseName][Optype] = 1\n",
        "    else:\n",
        "      userOperationDict[user][courseName] = dict()\n",
        "      userOperationDict[user][courseName][Optype] = 1\n",
        "  else:\n",
        "    userOperationDict[user] = dict()\n",
        "    userOperationDict[user][courseName] = dict()\n",
        "    userOperationDict[user][courseName][Optype] = 1\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSI3VoY9Ttv2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "courseDictList = list(courseDict.keys())\n",
        "\n",
        "output = open('/content/drive/My Drive/LDA Recommendation/courses1.csv','w')\n",
        "\n",
        "for index in range(len(courseDictList)):\n",
        "  output.write(courseDictList[index] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1fL7lNTXTIT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "courseInfoDict = dict()\n",
        "\n",
        "path_to_file = '/content/drive/My Drive/LDA Recommendation/course.txt'\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "    for line in f:\n",
        "      meta = json.loads(line)\n",
        "      #print(json.dumps(meta, sort_keys=True, indent=4))\n",
        "      courseInfoDict[meta['course_id'].replace('/','+')] = 1\n",
        "      break\n",
        "#print(len(courseInfoDict))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oC4DDU0oXW8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_file = '/content/drive/My Drive/LDA Recommendation/captionmap.json'\n",
        "\n",
        "captionDict = dict()\n",
        "\n",
        "with open(path_to_file) as f:\n",
        "  data = json.loads(f.read())\n",
        "  for key in data:\n",
        "    #print(key)\n",
        "    courseName = '+'.join(key.split('/')[2:4])\n",
        "    caption = data[key]\n",
        "    captionDict[courseName] = dict()\n",
        "    captionDict[courseName]['caption'] = caption"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXM8wi1Db0wR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "733a37aa-045e-4a56-bb55-fad0a0e18f90"
      },
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "import jieba.posseg\n",
        "import jieba.analyse\n",
        "import re\n",
        "\n",
        "jieba.analyse.set_stop_words('/content/drive/My Drive/LDA Recommendation/stopword.txt') \n",
        "\n",
        "def cut_words_with_pos(text):\n",
        "    seg = jieba.posseg.cut(text)\n",
        "    res = []\n",
        "    for i in seg:\n",
        "        if i.flag in [\"a\", \"v\", \"x\", \"n\", \"an\", \"vn\", \"nz\", \"nt\", \"nr\"] and is_fine_word(i.word):\n",
        "            res.append(i.word)\n",
        "    return list(res)\n",
        "  \n",
        "def is_fine_word(word, min_length=2):\n",
        "    rule = re.compile(r\"^[\\u4e00-\\u9fa5]+$\")\n",
        "    if len(word) >= min_length and re.search(rule, word):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "      \n",
        "sentences = []\n",
        "\n",
        "for key in captionDict:\n",
        "  caption = captionDict[key]['caption']\n",
        "  seg_list = cut_words_with_pos(caption.strip())\n",
        "  sentences.append(' '.join(seg_list))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.842 seconds.\n",
            "Prefix dict has been built succesfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gr0CCdSPGG_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0be5d8a0-a0e4-48c4-ec3e-acec9f69274f"
      },
      "cell_type": "code",
      "source": [
        "courseList = list(captionDict.keys())\n",
        "\n",
        "print(courseList)\n",
        "\n",
        "output = open('/content/drive/My Drive/LDA Recommendation/courses2.csv','w')\n",
        "\n",
        "for index in range(len(courseList)):\n",
        "  output.write(courseList[index] + '\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['TsinghuaX+80515522X', 'PEC+20220214X', 'TsinghuaX+10421102X', 'TSINGHUA+10610193X', 'TSINGHUA+20220214X', 'MITx+6_00_2x', 'GZUDS+DSA2015001', 'XJTUSPOC+CHEM2009', 'SDSZ+30700313x', 'TSINGHUA+10610224X', 'TsinghuaX+30240184_2015WV', 'SUDA+CAR101', 'PEC+20220214X_6', 'NJU+ELEC002', 'TSINGHUA+10430484_2015X', 'XJTUSPOC+COMP1023', 'USTC+10800032X', 'TSINGHUA+30640014X', 'FZU+CAH01', 'TSINGHUA+70120073_2015X', 'TsinghuaX+10800032_2015X', 'TsinghuaX+20240103X', 'USTC+00612642X', 'TsinghuaX+20250103X', 'TsinghuaX+00720091_2015X', 'JXUFE+30640014X', 'TsinghuaX+70240183x', 'NJU+ELEC003', 'TsinghuaX+30240184X', 'SEU+00690803', 'TsinghuaX+00690242_2X', 'TsinghuaX+60240013_2015X', 'WellesleyX+ANTH207x', 'MITx+15_390x', 'USTC+20220214X', 'TsinghuaX+AP000002X', 'TsinghuaX+80150193X', 'NEU+80000902X', 'HVMOOC+sict_01', 'JLU+DKW004', 'TsinghuaX+80000901_2X', 'shanximooc+ENG001', 'MITx+6_041x', 'shanximooc+CHEM2009', 'TsinghuaX+10421084X', 'TsinghuaX+00612642X', 'BIT+PHY1701702', 'JLU+DKW001', 'PEC+20220214X_8', 'XJTUSPOC+ELE001', 'XJTUSPOC+CS003', 'TsinghuaX+20220214X', 'TsinghuaX+34000888X', 'TsinghuaX+80000901X', 'TSINGHUA+30240184X', 'TsinghuaX+30240184_2X', 'XJTUSPOC+C00204', 'MITx+2_01x', 'TsinghuaSEM+80515702', 'TsinghuaX+10430896X', 'TsinghuaX+20440333_2015X', 'PEC+20220214_4X', 'TsinghuaX+40050455X', 'TsinghuaX+80000901_1X', 'NJU+ELEC001', 'RiceX+Phys102x', 'XJTUSPOC+MOOC102', 'TsinghuaX+00691153_2015X', 'WellesleyX+HIST229x', 'TsinghuaX+30240184_2015X', 'PEC+20220214X_7', 'MITx+6_002x', 'QLU+OC001', 'TsinghuaX+10421145X', 'TsinghuaX+20220214_2015X', 'FUDAN+FUDAN004', 'PEC+20220214X_9', 'TsinghuaX+10610183X', 'PEC+20220214_5X', 'EPFLx+BIO465_1x', 'ENG+30640014X_1', 'TsinghuaX+70120073_2015X', 'MITx+6_00_1x', 'MITx+15_390x_2015_T1', 'USTC+10610204X', 'ENG+30640014X_5', 'AdelaideX+humbio101x', 'TsinghuaX+00690242_WV_2015', 'TsinghuaX+30140393X', 'QLU+PEC2015001', 'TestVpc+efjefj737373_737733', 'USTC+MOOC003', 'TsinghuaX+70167012X', 'PEC+20220214_3X', 'TsinghuaX+10430484X', 'ENG+30640014X_3', 'TsinghuaX+00701032X', 'ENG+30640014X_2', 'TsinghuaX+30230931X', 'DavidsonX+001x', 'TsinghuaX+20440333_2014X', 'UQx+Think101x', 'TsinghuaX+30320174X', 'ZUEL+B0680090', 'TsinghuaX+20220332_1X', 'TestVpc+20220214X', 'UQx+BIOIMG101x', 'NEU+80000901X', 'TsinghuaX+30350161X', 'TestVpc+80512073X', 'TsinghuaX+34100325X', 'TsinghuaX+30140393_2015X', 'TsinghuaX+30640014X', 'SDSZ+80000901x', 'TsinghuaX+40260173X', 'TsinghuaX+30240184_1X', 'TsinghuaX+20440333X', 'TsinghuaX+00690342X', 'Stanford+WomensHealth_2015_T1', 'FUDAN+FUDAN003', 'TsinghuaX+60240202X', 'TSINGHUA+10430494_2015X', 'RiceX+AdvENVSCI_1x', 'SpocDemo+10610193X', 'TsinghuaX+10610193X', 'TsinghuaX+20120143X', 'TsinghuaX+20330334_2015X', 'UQx+TROPIC101x', 'Stanford+WomensHealth', 'SpocDemo+30240184X', 'TsinghuaX+30040323X', 'NNGLOBAL+DATA001', 'TsinghuaX+00691153X', 'TsinghuaSEM+70510773_2015', 'TSINGHUA+70250023X', 'USTC+00691153X', 'shanximooc+ARC001', 'FZU+CAR101', 'TsinghuaX+10450034_1X', 'TsinghuaX+40050444X', 'TsinghuaX+40670453X', 'TsinghuaX+20220053X', 'TsinghuaX+00680142X', 'edX+BlendedX', 'XJTUSPOC+MIT001', 'QLU+DW001', 'TsinghuaX+00720091_2014s_X', 'Sanyaxueyuan+MAYU2015001', 'NEU+00690242X', 'TsinghuaX+00690242_2015X', 'NJU+10430484_2015X', 'TSINGHUA+MOOC001', 'TestVpc+test001', 'TsinghuaX+20330334X', 'HNU+HNU001', 'NJU+CS102', 'TsinghuaX+00720091X', 'TsinghuaX+80512073X', 'TsinghuaX+00510888X', 'TsinghuaX+80512073_2015X', 'RiceX+AdvBIO_1x', 'shanximooc+MY001', 'TsinghuaX+10450012X', 'TsinghuaX+10430494X', 'TsinghuaX+20740084X', 'UC_BerkeleyX+CS_184_1x', 'SUDA+CUL001', 'shanximooc+FAN001', 'Tsinghuax+30130123X', 'TsinghuaX+20740042X', 'TsinghuaX+10421075_2015X', 'TsinghuaX+00690242_1X_2014', 'LZU+MOOC101', 'TsinghuaX+60250131X', 'TSINGHUA+10430484X', 'TsinghuaX+10421094X', 'TsinghuaX+70250023X', 'ENG+30640014X_4', 'HIT+30700313X', 'QHU+QHU0001', 'TsinghuaX+30130274X', 'NNGLOBAL+MOOC001', 'TsinghuaX+80512073_2015WV', 'TsinghuaX+20250064_2015X', 'TsinghuaX+00740113X', 'shanximooc+DLS001', 'TsinghuaX+00690092X', 'TsinghuaX+10430484_2015X', 'TestVpc+80000901X', 'TsinghuaX+10421084_2X_2015WV', 'RiceX+AdvBIO_2x', 'UC_BerkeleyX+CS169_1x_1', 'SDSZ+15_390x', 'FUDAN+20220214X', 'QHU+CS101', 'TsinghuaX+80000901_2015X', 'TsinghuaX+10610204X', 'SDSZ+CS101', 'TsinghuaX+20350033X', 'USTC+MOOC101', 'TsinghuaX+30240243X', 'NCTU+nctucmszb', 'HIT+00690242_2X', 'TsinghuaX+30700313_2015WV', 'TsinghuaX+70150023X', 'UQx+Write101_x', 'TsinghuaX+70120073X', 'BJ5HS+BJ5HS101', 'HVMOOC+80512073X', 'NCTU+DCT3082', 'XJTUSPOC+BIOL3028', 'TestVpc+MOOC101', 'shanximooc+MIT002', 'FZU+PSY01', 'TestVpc+15_390x_', 'SpocDemo+10430484_2015X', 'HIT+15_390x', 'USTC+80000901X', 'PEC+20220214_2X', 'TsinghuaX+34000312X', 'TsinghuaX+10800032X', 'TsinghuaX+THUSEM002', 'BIT+PHY1701701', 'TsinghuaX+60240013X', 'TsinghuaX+20220332_2X', 'MITx+2_03x', 'TsinghuaX+80000901_2015WV', 'TsinghuaX+30150153X', 'TsinghuaX+00612642_2015WV', 'RiceX+BIOC372_1x', 'TsinghuaX+THESIS2014_1X', 'TsinghuaX+10421065X', 'TSINGHUA+30140393_2015X', 'BerkeleyX+CS169_1x', 'JLU+DKW002', 'NEU+00690241X', 'UC_BerkeleyX+ColWri2_1x_2015WV', 'TsinghuaX+01030132X_2015X', 'USTC+MOOC002', 'TsinghuaX+01030132X', 'TsinghuaSEM+70510773', 'TsinghuaX+60240013_2014X', 'TsinghuaX+30230931_2015X', 'shanximooc+C00204', 'TsinghuaX+70167012_2015X', 'TSINGHUA+20250064X', 'TsinghuaX+00691153X_2015WV', 'TsinghuaX+30640014_2015X', 'TsinghuaX+00612642_2015X', 'TsinghuaX+AP000004X', 'TsinghuaX+70250023_2015X', 'TSINGHUA+60240013X', 'TsinghuaX+30700313X', 'SEMS+LIKE150501', 'TsinghuaX+30240233X', 'TsinghuaX+30700313_2014X', 'ThtfSEM+THTF_2_01', 'TsinghuaSEM+80512073', 'TsinghuaX+10610224X', 'TsinghuaX+THUSEM009', 'TsinghuaX+00680082X', 'QHU+CS102', 'TsinghuaX+THU00001X', 'TsinghuaX+10421084_2015X', 'XJTUSPOC+ENG101', 'TsinghuaX+20250064_1X', 'TsinghuaX+10421145_2015X', 'shanximooc+COMP1023', 'TsinghuaX+40130653X', 'TsinghuaX+10421094_2015X', 'TsinghuaX+60510102X', 'TsinghuaX+AP000003X', 'QLU+DW002', 'TsinghuaX+40050455_2015X', 'TsinghuaSEM+70510133', 'TsinghuaX+30700313_2015X', 'TsinghuaSEM+80515341', 'ZUEL+B0680090_2015_T1', 'TsinghuaX+10421084_2015WV', 'TsinghuaX+AP000000X', 'TsinghuaX+10610204_2015X', 'TsinghuaX+10430494_2015X', 'USTC+30700313X', 'NJU+CS101', 'TsinghuaX+10421094X_2015wv', 'TsinghuaX+00690242_1X', 'TsinghuaX+20140064X', 'TsinghuaX+MOOC101_2015', 'QHU+CS103', 'TsinghuaX+00720091_2014X', 'TsinghuaX+10421084_2014X', 'SEMS+LIKE150502', 'TSINGHUA+TAC001', 'TsinghuaX+30260112X', 'TsinghuaX+00740043X', 'TsinghuaX+20250064X', 'SpocDemo+20250064X', 'BJ5HS+CS101', 'TSINGHUA+60250131X', 'BIT+ELC05198', 'TsinghuaX+40040152X', 'TsinghuaX+70250023_2014X', 'TsinghuaX+MOOC101_2014', 'TsinghuaX+80512073_2014_1X', 'RiceX+BIOC372_2x', 'RiceX+AdvENVSCI_3x', 'HIT+00690242_1X', 'UC_BerkeleyX+ColWri2_3x_2015_T1', 'TsinghuaX+10421084_2X', 'TsinghuaX+80512073_2014_2X', 'UC_BerkeleyX+ColWri2_2x_2015_T1', 'TsinghuaX+AP000005X', 'TSINGHUA+70167012X', 'UC_BerkeleyX+ColWri2_1x_2015_T1', 'FZU+MOOC01', 'TsinghuaX+THUSEM001', 'NJU+EE003', 'RiceX+AdvENVSCI_2x', 'ThtfSEM+THTF_2_02', 'UC_BerkeleyX+ColWri2_3x', 'NEU+NEU101', 'TsinghuaX+80511503X', 'UC_BerkeleyX+ColWri2_1x', 'UC_BerkeleyX+ColWri2_2x', 'UC_BerkeleyX+ColWri2_2x_2015WV', 'TsinghuaX+THUSEM010', 'QHU+00740043', 'TsinghuaX+00740043_2X', 'TestVpc+xuetang_test_01', 'UniversityX+CS102', 'JXUFE+17424']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KffoGEkAgfQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip uninstall pbr\n",
        "#!pip uninstall lda\n",
        "#!pip install lda\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWOhQxC2JXwg",
        "colab_type": "code",
        "outputId": "63bad9ba-a27b-4a22-ea41-16ff58c4c957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import feature_extraction  \n",
        "from sklearn.feature_extraction.text import TfidfTransformer  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import seaborn as sns\n",
        "import lda\n",
        "\n",
        "corpus = sentences\n",
        "  \n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "weight = X.toarray()\n",
        "\n",
        "def plot_topic_words(topic_word):\n",
        "    plt.figure(figsize=(8, 9))\n",
        "    # f, ax = plt.subplots(5, 1, sharex=True)\n",
        "    for i, k in enumerate([0, 5, 9, 14, 19]):\n",
        "        ax = plt.subplot(5, 1, i+1)\n",
        "        ax.plot(topic_word[k, :], 'r-')\n",
        "        ax.set_xlim(-50, 4350)   # [0,4258]\n",
        "        ax.set_ylim(0, 0.08)\n",
        "        ax.set_ylabel(u\"概率\")\n",
        "        ax.set_title(u\"主题 {}\".format(k))\n",
        "    plt.xlabel(u\"词\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(u'主题的词分布', fontsize=18)\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.show()\n",
        "\n",
        "def plot_topic(doc_topic):\n",
        "    f, ax = plt.subplots(figsize=(30, 4))\n",
        "    cmap = sns.cubehelix_palette(start=1, rot=3, gamma=0.8, as_cmap=True)\n",
        "    sns.heatmap(doc_topic.T, linewidths=0.05, ax=ax)\n",
        "    ax.set_title('Chat Topic in Each Course')\n",
        "    ax.set_xlabel('Stage')\n",
        "    ax.set_ylabel('Topic')\n",
        "    plt.show()\n",
        "    #f.savefig('output/topic_heatmap.jpg', bbox_inches='tight')\n",
        "\n",
        "\n",
        "def lda_train(weight, vectorizer):\n",
        "    model = lda.LDA(n_topics=50, n_iter=100, random_state=1)\n",
        "    model.fit(weight)\n",
        "\n",
        "    doc_num = len(weight)\n",
        "    topic_word = model.topic_word_\n",
        "    vocab = vectorizer.get_feature_names()\n",
        "    titles = [\"Stage{}\".format(i) for i in range(1, doc_num + 1)]\n",
        "\n",
        "    n_top_words = 20\n",
        "    #for i, topic_dist in enumerate(topic_word):\n",
        "      #print(np.sort(topic_dist))\n",
        "        #topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
        "        #print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
        "\n",
        "    doc_topic = model.doc_topic_\n",
        "    \n",
        "    for index in range(len(doc_topic)):\n",
        "      score = doc_topic[index]\n",
        "      course = courseList[index]\n",
        "      captionDict[course]['score'] = score\n",
        "   \n",
        "    #print(doc_topic, type(doc_topic))\n",
        "    #plot_topic(doc_topic)\n",
        "    #for i in range(doc_num):\n",
        "    #    print(\"{} (top topic: {})\".format(titles[i], np.argsort(doc_topic[i])[:-4:-1]))\n",
        "\n",
        "lda_train(weight, vectorizer)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:lda:n_documents: 331\n",
            "INFO:lda:vocab_size: 12710\n",
            "INFO:lda:n_words: 99244\n",
            "INFO:lda:n_topics: 50\n",
            "INFO:lda:n_iter: 100\n",
            "WARNING:lda:all zero row in document-term matrix found\n",
            "INFO:lda:<0> log likelihood: -1370782\n",
            "INFO:lda:<10> log likelihood: -835754\n",
            "INFO:lda:<20> log likelihood: -804056\n",
            "INFO:lda:<30> log likelihood: -789096\n",
            "INFO:lda:<40> log likelihood: -780097\n",
            "INFO:lda:<50> log likelihood: -774563\n",
            "INFO:lda:<60> log likelihood: -770343\n",
            "INFO:lda:<70> log likelihood: -766041\n",
            "INFO:lda:<80> log likelihood: -763492\n",
            "INFO:lda:<90> log likelihood: -761279\n",
            "INFO:lda:<99> log likelihood: -758456\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "z4-opXUJ8_36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "53491434-ef03-4cca-b4be-5e3efae4a05f"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "watchWeight = 1\n",
        "infoWeight = 0.3\n",
        "\n",
        "userTopicVectorDict = dict()\n",
        "\n",
        "def vecAdd(vec1, vec2):\n",
        "  \n",
        "  vec3 = vec1\n",
        "  len1 = len(vec1)\n",
        "  len2 = len(vec2)\n",
        "  if len1 == len2:\n",
        "    for i in range(len1):\n",
        "      vec3[i] += vec2[i]\n",
        "  else:\n",
        "    print(len1, len2)\n",
        "  return vec3\n",
        "\n",
        "def vecMutify(vec1, scale):\n",
        "  \n",
        "  vec2 = vec1\n",
        "  len1 = len(vec1)\n",
        "  \n",
        "  for i in range(len1):\n",
        "    vec2[i] *= scale\n",
        "\n",
        "  return vec2\n",
        "\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "for user in userOperationDict:\n",
        "  \n",
        "  userTopicVector = []\n",
        "  \n",
        "  for i in range(50):\n",
        "    userTopicVector.append(0)\n",
        "    \n",
        "  userTopicVectorDict[user] = userTopicVector\n",
        "\n",
        "  for course in userOperationDict[user]:\n",
        "    \n",
        "    watchNum = 0\n",
        "    if 'watch' in userOperationDict[user][course]:\n",
        "      watchNum = userOperationDict[user][course]['watch']\n",
        "      \n",
        "    introNum = 0\n",
        "    if 'intro' in userOperationDict[user][course]:\n",
        "      introNum = userOperationDict[user][course]['intro']\n",
        "    \n",
        "    #print(course)\n",
        "    if course in captionDict:\n",
        "      \n",
        "      scale = watchNum * watchWeight + introNum * infoWeight\n",
        "      \n",
        "      courseTopicVector = vecMutify(captionDict[course]['score'], scale)\n",
        "      #print(userTopicVectorDict[user])\n",
        "      userTopicVectorDict[user] = vecAdd(userTopicVectorDict[user], courseTopicVector)\n",
        "    \n",
        "for user in userTopicVectorDict:\n",
        "  \n",
        "  vecSum = 0\n",
        "  \n",
        "  for index in range(50):\n",
        "    vecSum += userTopicVectorDict[user][index]\n",
        "  \n",
        "  if vecSum > 0:\n",
        "    x = userTopicVectorDict[user]\n",
        "    userTopicVectorDict[user] = normalized(x)\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning:\n",
            "\n",
            "overflow encountered in double_scalars\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py:2286: RuntimeWarning:\n",
            "\n",
            "overflow encountered in multiply\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: RuntimeWarning:\n",
            "\n",
            "overflow encountered in double_scalars\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py:2287: RuntimeWarning:\n",
            "\n",
            "overflow encountered in reduce\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BrNaGEOYZeoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a5559d55-3dfe-488b-c897-7357f70ebe24"
      },
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "distanceDict = dict()\n",
        "\n",
        "userNum = len(userTopicVectorDict)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for userA in userTopicVectorDict:\n",
        "  \n",
        "  counter += 1\n",
        "  \n",
        "  if counter % 100 == 0:\n",
        "    print(counter / userNum)\n",
        "    \n",
        "  for userB in userTopicVectorDict:\n",
        "    if userA != userB:\n",
        "      \n",
        "      head = userA + '|' + userB\n",
        "      vecA = userTopicVectorDict[userA]\n",
        "      vecB = userTopicVectorDict[userB]\n",
        "      \n",
        "      sim = 1 - spatial.distance.cosine(vecA, vecB)\n",
        "      \n",
        "      distanceDict[head] = sim\n",
        "      \n",
        "print(distanceDict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:698: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in double_scalars\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "61a-RfyqK4gp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = open('/content/drive/My Drive/LDA Recommendation/simResult.csv','w')\n",
        "\n",
        "for pair in distanceDict:\n",
        "  val = distanceDict[pair]\n",
        "  userA = pair.split('|')[0]\n",
        "  userB = pair.split('|')[1]\n",
        "  output.write(userA + ',' + userB + ',' + str(val) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}